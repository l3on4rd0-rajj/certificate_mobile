#!/usr/bin/env python3
"""
Consulta endoflife.date para cada biblioteca listada em um CSV
usando **/products/{product}**. Caso o slug direto não exista, resolve
através dos catálogos de identificadores (`/identifiers/{type}`).

Versão **4.0** – slug resolver + Excel/CSV
-----------------------------------------
* Tenta slug gerado via `slugify`.
* Se 404, percorre `IDENTIFIER_TYPES` (`npm`, `pypi`, `maven`, `composer`,
  `gem`, `go`, `crates`, `repology`) e procura o *name* nos mapeamentos
  `<slug → packageName>` devolvendo o slug correspondente.
* Caches cada dicionário para evitar múltiplas requisições.
* Gera `resultado-eol.csv` e `resultado-eol.xlsx`.
* Suporta `--proxy`, `--insecure`, `--debug`.

Dependências:
```bash
pip install pandas requests openpyxl python-slugify tqdm
```
"""

from __future__ import annotations

import argparse
import datetime as dt
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Any, Optional

import pandas as pd
import requests

# --- slugify fallback -------------------------------------------------------
try:
    from slugify import slugify as _slugify  # type: ignore
except ModuleNotFoundError:
    import re

    def _slugify(text: str) -> str:
        text = re.sub(r"[^a-z0-9]+", "-", text.lower())
        return text.strip("-")

slugify = _slugify

# --- tqdm fallback ----------------------------------------------------------
try:
    from tqdm import tqdm  # type: ignore
except ModuleNotFoundError:

    def tqdm(iterable, total=None):  # type: ignore
        return iterable

# === Configurações globais ==================================================
BASE = "https://endoflife.date/api/v1"
TODAY = dt.date.today()
DEFAULT_THREADS = 8
TIMEOUT = 10
INPUT_CSV = Path("libs_comuns.csv")
OUTPUT_CSV = Path("resultado-eol.csv")
DEFAULT_COLUMN = None
IDENTIFIER_TYPES = [
    "npm",
    "pypi",
    "maven",
    "composer",
    "gem",
    "go",
    "crates",
    "repology",
]

session = requests.Session()
identifier_cache: Dict[str, Dict[str, str]] = {}


# === HTTP helpers ===========================================================

def configure_http(proxy: Optional[str], insecure: bool) -> None:
    if proxy:
        session.proxies.update({"http": proxy, "https": proxy})
    elif os.getenv("HTTPS_PROXY"):
        session.proxies.update({"http": os.getenv("HTTPS_PROXY"), "https": os.getenv("HTTPS_PROXY")})
    session.verify = not insecure
    if insecure:
        requests.packages.urllib3.disable_warnings()  # type: ignore


def _request(path: str, debug=False) -> Optional[dict]:
    try:
        resp = session.get(f"{BASE}{path}", timeout=TIMEOUT)
        if debug:
            print(f"{path} -> HTTP {resp.status_code}", file=sys.stderr)
        if resp.ok:
            return resp.json()
    except requests.RequestException as exc:
        if debug:
            print(f"REQ ERROR {path}: {exc}", file=sys.stderr)
    return None

# === Identificador resolver ================================================

def load_identifier_map(id_type: str, debug=False) -> Dict[str, str]:
    """Carrega (e cacheia) o mapa slug→package para o tipo fornecido."""
    if id_type in identifier_cache:
        return identifier_cache[id_type]
    data = _request(f"/identifiers/{id_type}", debug) or {}
    identifier_cache[id_type] = data
    return data


def resolve_slug(name: str, debug=False) -> Optional[str]:
    """Procura o *name* entre todos os identificadores e devolve o slug."""
    for id_type in IDENTIFIER_TYPES:
        mapping = load_identifier_map(id_type, debug)
        for slug, pkg_name in mapping.items():
            # comparação case-insensitive e sem espaços/pontos extras
            if pkg_name.lower() == name.lower():
                if debug:
                    print(f"slug resolved via {id_type}: {name} -> {slug}", file=sys.stderr)
                return slug
    return None

# === Product fetch/flatten ==================================================

def fetch_product(slug: str, debug=False) -> Optional[dict]:
    prod = _request(f"/products/{slug}", debug)
    if not prod or "result" not in prod:
        return None
    return prod["result"]


def flatten_product(data: dict) -> dict:
    labels = data.get("labels", {})
    releases = data.get("releases", [])
    maintained = next((r for r in releases if r.get("isMaintained")), None)
    latest = maintained or releases[0] if releases else {}
    return {
        "category": data.get("category"),
        "eol_label": labels.get("eol"),
        "eoas_label": labels.get("eoas"),
        "discontinued": labels.get("discontinued"),
        "current_cycle": latest.get("name") or latest.get("label"),
        "current_latest_version": latest.get("latest", {}).get("name"),
        "current_eol": latest.get("eolFrom"),
    }

# === Worker =================================================================

def process(name: str, debug=False) -> Dict[str, Any]:
    direct_slug = slugify(name)
    data = fetch_product(direct_slug, debug)

    if not data:
        # tentar resolver via identificadores
        resolved = resolve_slug(name, debug)
        if resolved and resolved != direct_slug:
            data = fetch_product(resolved, debug)
            slug_used = resolved
        else:
            slug_used = direct_slug
    else:
        slug_used = direct_slug

    if not data:
        return {"biblioteca": name, "slug": slug_used, "status": "não encontrado"}

    flat = flatten_product(data)
    return {
        "biblioteca": name,
        "slug": slug_used,
        **flat,
        "json": data,
    }

# === IO helpers =============================================================

def read_libs(path: Path, column: str | None) -> List[str]:
    df = pd.read_csv(path)
    if column is None:
        column = df.columns[0]
    if column not in df.columns:
        raise ValueError(f"Coluna '{column}' não encontrada no CSV.")
    return df[column].dropna().astype(str).unique().tolist()

# === CLI ====================================================================

def main() -> None:
    p = argparse.ArgumentParser(description="Gera CSV/XLSX com suporte de bibliotecas via endoflife.date")
    p.add_argument("-i", "--input")
    p.add_argument("-o", "--output")
    p.add_argument("-c", "--column", default=DEFAULT_COLUMN)
    p.add_argument("-t", "--threads", type=int, default=DEFAULT_THREADS)
    p.add_argument("--proxy")
    p.add_argument("--insecure", action="store_true")
    p.add_argument("--debug", action="store_true")
    args = p.parse_args()

    configure_http(args.proxy, args.insecure)

    input_path = Path(args.input) if args.input else INPUT_CSV
    output_base = Path(args.output) if args.output else OUTPUT_CSV

    try:
        libs = read_libs(input_path, args.column)
    except Exception as exc:
        print(f"Erro lendo CSV: {exc}", file=sys.stderr)
        sys.exit(1)

    results: List[Dict[str, Any]] = []
    with ThreadPoolExecutor(max_workers=args.threads) as pool:
        futures = {pool.submit(process, name, args.debug): name for name in libs}
        for fut in tqdm(as_completed(futures), total=len(libs)):
            name = futures[fut]
            try:
                results.append(fut.result())
            except Exception as exc:
                results.append({"biblioteca": name, "slug": slugify(name), "status": f"erro: {exc}"})

    df = pd.json_normalize(results, sep=".")
    df.to_csv(output_base.with_suffix(".csv"), index=False)
    df.to_excel(output_base.with_suffix(".xlsx"), index=False, engine="openpyxl")
    print(f"✅ Resultado salvo em: {output_base.with_suffix('.xlsx')}")


if __name__ == "__main__":
    main()
